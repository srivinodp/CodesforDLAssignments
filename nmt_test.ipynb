{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A4_T3test.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnxXKDjq3jEL",
        "outputId": "2835d760-59d7-43a6-c4ea-7ea682547c72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5HDjDOxJe1k",
        "outputId": "afcf399b-8119-4925-b402-1c0cc7e6092b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRAw0CYIWiJL",
        "outputId": "e234bad6-57d7-4c19-d190-918b792e6d7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "embeddings_index = dict()\n",
        "f = open('/content/drive/My Drive/glove.6B.300d.txt',encoding=\"utf-8\")\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzd3_VtGYMyk"
      },
      "source": [
        "docs=embeddings_index.keys()\n",
        "tokenizer_en = Tokenizer(num_words=400000)\n",
        "tokenizer_en.fit_on_texts(docs)\n",
        "vocab_size = len(tokenizer_en.word_index) + 3\n",
        "embedding_matrix = np.zeros((vocab_size, 300))\n",
        "for word, i in tokenizer_en.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWU693nad9dz"
      },
      "source": [
        "train_dataset_en=tf.data.TextLineDataset('/content/drive/My Drive/DEEP LEARNING ASSIGNMENTS/A4_set2/train_en.txt')\n",
        "train_dataset_ta=tf.data.TextLineDataset('/content/drive/My Drive/DEEP LEARNING ASSIGNMENTS/A4_set2/train_ta.txt')\n",
        "train_dataset_en=[str(i.decode('utf-8')).replace('\\'',' \\'') for i in train_dataset_en.as_numpy_iterator()]\n",
        "train_dataset_en=tf.data.Dataset.from_tensor_slices(train_dataset_en)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMj2KeZbY-wu"
      },
      "source": [
        "tokenizer_ta= tfds.features.text.SubwordTextEncoder.build_from_corpus((element for element in train_dataset_ta.as_numpy_iterator()), target_vocab_size=10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K10pi19Ueo8X"
      },
      "source": [
        "vocab_size = len(tokenizer_en.word_index) + 3\n",
        "vocab_tar_size = len(tokenizer_ta.word_index) + 3\n",
        "units = 1024\n",
        "BATCH_SIZE = 64\n",
        "embedding_dim = 256\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZ2rI24i3jFg"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_matrix, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, 300,embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),trainable=False)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJ_B3mhW3jFk"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    # self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    # context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    # x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x,initial_state = hidden)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhM2m819U9_q"
      },
      "source": [
        "encoder = Encoder(vocab_size, embedding_matrix, units, BATCH_SIZE)\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(tf.zeros((BATCH_SIZE,20)),sample_hidden)\n",
        "encoder.load_weights('/content/drive/My Drive/DEEP LEARNING ASSIGNMENTS/A4_set2/Copy of encoder_new.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfsvqlhiU-RM"
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "sample_decoder_output, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "decoder.load_weights('/content/drive/My Drive/DEEP LEARNING ASSIGNMENTS/A4_set2/Copy of decoder_new.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbQpyYs13jF_"
      },
      "source": [
        "def evaluate(sentence,max_length_targ=100):\n",
        "#   attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "  start_token = [len(tokenizer_en.word_index)+1]\n",
        "  end_token = [len(tokenizer_en.word_index) + 2]\n",
        "  \n",
        "  st=tokenizer_en.texts_to_sequences([sentence.lower()])\n",
        "  flat_list = [item for sublist in st for item in sublist]\n",
        "  inp_sentence = start_token + flat_list + end_token\n",
        "  encoder_input = tf.expand_dims(inp_sentence, 0)\n",
        "  \n",
        "  result = []\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(encoder_input, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([vocab_tar_size-2] , 0)\n",
        "\n",
        "  for t in range(max_length_targ):\n",
        "    predictions, dec_hidden = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    # attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    # attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    \n",
        "    result += [predicted_id]\n",
        "    \n",
        "    if predicted_id == vocab_tar_size-1:\n",
        "      return result, sentence\n",
        "    \n",
        "    \n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3IOI9kxSSaP"
      },
      "source": [
        "def translate(sentence, plot=''):\n",
        "  result,sentence = evaluate(sentence.replace('\\'',' \\''))\n",
        "  predicted_sentence = tokenizer_ta.sequences_to_texts([result.numpy()[1:]])  \n",
        "  return predicted_sentence[0]\n",
        "  \n",
        "#   predicted_sentence = tokenizer_ta.decode([i for i in result \n",
        "#                                             if i <= tokenizer_ta.vocab_size])\n",
        "#   return predicted_sentence  \n",
        "#   print(result)\n",
        "#   print('Input: {}'.format(sentence))\n",
        "#   print('Predicted translation: {}'.format(predicted_sentence))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hj1msVTrVNXC",
        "outputId": "883d42d6-c299-460c-bf2e-cea3130b0bdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "translate(\"That's where we're going.\")\n",
        "print (\"நாம் எங்கே போகிறோம் என்று.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "எங்காவது எங்கும் நாம் எங்கே.\n",
            "நாம் எங்கே போகிறோம் என்று.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LuIU5KgDerQ",
        "cellView": "code"
      },
      "source": [
        "test_dataset_en=tf.data.TextLineDataset('/content/drive/My Drive/DEEP LEARNING ASSIGNMENTS/A4_set2/dev_en.txt')\n",
        "test_dataset_ta=tf.data.TextLineDataset('/content/drive/My Drive/DEEP LEARNING ASSIGNMENTS/A4_set2/dev_ta.txt')\n",
        "\n",
        "test_dataset_en=[str(i.decode('utf-8')).replace('\\'',' \\'') for i in test_dataset_en.as_numpy_iterator()]\n",
        "test_dataset_en=tf.data.Dataset.from_tensor_slices(test_dataset_en)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMH9Y7WyGGuB"
      },
      "source": [
        "test_dataset = tf.data.Dataset.zip((test_dataset_en, test_dataset_ta))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tvsEVs0u0LU"
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "def bleu_score_function(reference,candidate):\n",
        "  bleu1=sentence_bleu([reference], candidate,weights=(1, 0, 0, 0))\n",
        "  bleu2=sentence_bleu([reference], candidate,weights=(0.5, 0.5, 0, 0))\n",
        "  bleu3=sentence_bleu([reference], candidate,weights=(0.33, 0.33, 0.33, 0))\n",
        "  bleu4=sentence_bleu([reference], candidate,weights=(0.25, 0.25, 0.25, 0.25))\n",
        "  return bleu1,bleu2,bleu3,bleu4\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-806lDjFrpiR",
        "outputId": "8cf691c3-9373-4c88-ecbb-627c818a446d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        }
      },
      "source": [
        "scores1=[]\n",
        "scores2=[]\n",
        "scores3=[]\n",
        "scores4=[]\n",
        "for(inp,tar) in test_dataset:\n",
        "  prediction=translate(inp.numpy().decode('utf-8').lower().replace('\\'',' \\''))\n",
        "  print(inp.numpy())\n",
        "  reference=tar.numpy().decode('utf-8').split(' ')\n",
        "  candidate=prediction.split(' ')\n",
        "  print(reference)\n",
        "  print(candidate)\n",
        "  bleu1,bleu2,bleu3,bleu4 = bleu_score_function(reference, candidate)\n",
        "  print(bleu1)\n",
        "  print(bleu2)\n",
        "  print(bleu3)\n",
        "  print(bleu4)\n",
        "  scores1.append(bleu1)\n",
        "  scores2.append(bleu2)\n",
        "  scores3.append(bleu3)\n",
        "  scores4.append(bleu4)\n",
        "  break\n",
        "\n",
        "\n",
        "print(sum(scores1)/len(scores1))\n",
        "print(sum(scores2)/len(scores2))\n",
        "print(sum(scores3)/len(scores3))\n",
        "print(sum(scores4)/len(scores4))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b\"You will tell us Cobra 's endgame, or die by the same sword you once used to kill our master.\"\n",
            "['நீங்கள்', 'எங்களுக்கு', 'கோப்ரா', 'என்ற', 'எண்ட்கேமை', 'சொல்கிறேன்,', 'அல்லது', 'Die', 'நீ', 'நம்', 'மாஸ்டர்', 'கொல்ல', 'பயன்படுத்தப்பட்ட', 'அதே', 'வாள்.']\n",
            "['நீங்கள்', 'எங்கள்', 'சில', 'முடிவுக்ள', 'அடியை', 'பார்க்க', 'உதவியும்', 'கொல்லவில்லை', 'உள்ளது?']\n",
            "0.05704634655917688\n",
            "2.552826540650872e-155\n",
            "2.212165637760313e-204\n",
            "5.400301927028362e-232\n",
            "0.05704634655917688\n",
            "2.552826540650872e-155\n",
            "2.212165637760313e-204\n",
            "5.400301927028362e-232\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}