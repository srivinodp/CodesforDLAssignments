{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "nmt_without_attention.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnxXKDjq3jEL"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5HDjDOxJe1k",
        "outputId": "d24e773e-f97e-4941-9495-6d7553157392",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRAw0CYIWiJL",
        "outputId": "4537a137-3970-4a63-9604-63679162e8a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "embeddings_index = dict()\n",
        "f = open('/content/drive/My Drive/glove.6B.300d.txt',encoding=\"utf-8\")\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzd3_VtGYMyk"
      },
      "source": [
        "docs=embeddings_index.keys()\n",
        "tokenizer_en = Tokenizer(num_words=400000)\n",
        "tokenizer_en.fit_on_texts(docs)\n",
        "vocab_size = len(tokenizer_en.word_index) + 3\n",
        "embedding_matrix = np.zeros((vocab_size, 300))\n",
        "for word, i in tokenizer_en.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWU693nad9dz"
      },
      "source": [
        "train_dataset_en=tf.data.TextLineDataset('/content/drive/My Drive/DEEP LEARNING ASSIGNMENTS/A4_set2/train_en.txt')\n",
        "train_dataset_ta=tf.data.TextLineDataset('/content/drive/My Drive/DEEP LEARNING ASSIGNMENTS/A4_set2/train_ta.txt')\n",
        "train_dataset_en=[str(i.decode('utf-8')).replace('\\'',' \\'') for i in train_dataset_en.as_numpy_iterator()]\n",
        "train_dataset_en=tf.data.Dataset.from_tensor_slices(train_dataset_en)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxjFlNAWQcDg"
      },
      "source": [
        "# print(train_dataset_en.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMj2KeZbY-wu"
      },
      "source": [
        "tokenizer_ta= tfds.features.text.SubwordTextEncoder.build_from_corpus((element for element in train_dataset_ta.as_numpy_iterator()), target_vocab_size=10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K10pi19Ueo8X"
      },
      "source": [
        "vocab_size = len(tokenizer_en.word_index) + 3\n",
        "vocab_tar_size = tokenizer_ta.vocab_size + 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8AHnteLeB0N",
        "outputId": "5bb89fb6-b49c-4bac-bf17-67bdfed38c7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "train_dataset = tf.data.Dataset.zip((train_dataset_en, train_dataset_ta))\n",
        "print(train_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<ZipDataset shapes: ((), ()), types: (tf.string, tf.string)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGUWBU2z3C1_",
        "outputId": "5dcc020f-953e-49dd-83fa-283685c4ca1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "train_sentences = 0\n",
        "for (batch,(inp,tar)) in enumerate(train_dataset):\n",
        "    #print(inp)\n",
        "    #break\n",
        "    train_sentences += 1\n",
        "print(train_sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26217\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2oStMc0ZaDe"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "def convertToSequence(lang1, lang2):\n",
        "    st=tokenizer_en.texts_to_sequences([lang1.numpy().decode('utf-8').lower()])\n",
        "    flat_list = [item for sublist in st for item in sublist]\n",
        "\n",
        "    lang1 = [len(tokenizer_en.word_index)+1] + flat_list + [len(tokenizer_en.word_index)+2]\n",
        "\n",
        "    lang2 = [tokenizer_ta.vocab_size+1] + tokenizer_ta.encode(lang2.numpy()) + [tokenizer_ta.vocab_size+2]\n",
        "\n",
        "    return lang1, lang2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4MI6hHhZgeE"
      },
      "source": [
        "def toSequence(en, ta):\n",
        "    result_en, result_ta = tf.py_function(convertToSequence, [en, ta], [tf.int64, tf.int64])\n",
        "    result_en.set_shape([None])\n",
        "    result_ta.set_shape([None])\n",
        "\n",
        "    return result_en, result_ta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yxhe_gqZlOw",
        "outputId": "35395443-d284-4e00-eafc-c984d5e17b68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "source": [
        "train_dataset_indices = train_dataset.map(toSequence)\n",
        "padded_train_dataset = train_dataset_indices.padded_batch(BATCH_SIZE,padded_shapes=([None],[None]))\n",
        "for (batch,(inp,tar)) in enumerate(padded_train_dataset):\n",
        "    print(inp)\n",
        "    print(tar)\n",
        "    break\n",
        "# print(len(input_tensor),len(input_tensor))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[339252   6098  13530 ...      0      0      0]\n",
            " [339252   6101  13573 ...      0      0      0]\n",
            " [339252   3371  24713 ...      0      0      0]\n",
            " ...\n",
            " [339252   4232   3373 ...      0      0      0]\n",
            " [339252   4243    222 ...      0      0      0]\n",
            " [339252   3371   4290 ...      0      0      0]], shape=(64, 23), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[4410   10    6 ...    0    0    0]\n",
            " [4410   42    1 ...    0    0    0]\n",
            " [4410   10   26 ...    0    0    0]\n",
            " ...\n",
            " [4410   19    6 ...    0    0    0]\n",
            " [4410   52    1 ...    0    0    0]\n",
            " [4410   10   26 ...    0    0    0]], shape=(64, 58), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZ2rI24i3jFg"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_matrix, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, 300,embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),trainable=False)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60gSVh05Jl6l",
        "outputId": "32668178-e532-40db-eb26-8d593fcad4d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "units = 1024\n",
        "BATCH_SIZE = 64\n",
        "encoder = Encoder(vocab_size, embedding_matrix, units, BATCH_SIZE)\n",
        "print(encoder.trainable_variables)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "# for (batch,(inp,tar)) in enumerate(train_dataset_indices):\n",
        "#     t = inp\n",
        "    # e = tf.keras.layers.Embedding(vocab_size, 300,embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),trainable=False)\n",
        "    # g = tf.keras.layers.GRU(4)\n",
        "    # print(inp,g([b+for e(inp)]))\n",
        "    # sample_output, sample_hidden = encoder(inp, sample_hidden)\n",
        "    # print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "    # print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))\n",
        "    # break\n",
        "for (batch,(inp,tar)) in enumerate(padded_train_dataset):\n",
        "    sample_output, sample_hidden = encoder(inp, sample_hidden)\n",
        "    print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "    print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))\n",
        "    break\n",
        "    # print(t.shape)\n",
        "# print(batch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n",
            "Encoder output shape: (batch size, sequence length, units) (64, 23, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJ_B3mhW3jFk"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    # self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    # context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    # x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x,initial_state = hidden)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5UY8wko3jFp",
        "outputId": "20ef4180-e2c1-444c-c409-92a7c27d64ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "embedding_dim = 256\n",
        "print(sample_hidden.shape, sample_output.shape)\n",
        "# decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample_decoder_output, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "#                                       sample_hidden, sample_output)\n",
        "\n",
        "# print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 1024) (64, 23, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmTHr5iV3jFr"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMVWzzsfNl4e"
      },
      "source": [
        "## Checkpoints (Object-based saving)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpObfY22IddU"
      },
      "source": [
        "## Training\n",
        "\n",
        "1. Pass the *input* through the *encoder* which return *encoder output* and the *encoder hidden state*.\n",
        "2. The encoder output, encoder hidden state and the decoder input (which is the *start token*) is passed to the decoder.\n",
        "3. The decoder returns the *predictions* and the *decoder hidden state*.\n",
        "4. The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n",
        "5. Use *teacher forcing* to decide the next input to the decoder.\n",
        "6. *Teacher forcing* is the technique where the *target word* is passed as the *next input* to the decoder.\n",
        "7. The final step is to calculate the gradients and apply it to the optimizer and backpropagate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sC9ArXSsVfqn"
      },
      "source": [
        "\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([tokenizer_ta.vocab_size+1] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "  train_accuracy(targ[:, t], predictions)\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddefjBMa3jF0",
        "outputId": "97bf79f7-7d0c-48d2-db37-4278b2fec0d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "EPOCHS = 20\n",
        "steps_per_epoch = train_sentences//BATCH_SIZE\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(padded_train_dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f} Accuracy {:.3f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy(),train_accuracy.result()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "#   if (epoch + 1) % 2 == 0:\n",
        "#     checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 3.8639 Accuracy 0.000\n",
            "Epoch 1 Batch 100 Loss 0.8215 Accuracy 0.009\n",
            "Epoch 1 Batch 200 Loss 0.9243 Accuracy 0.011\n",
            "Epoch 1 Batch 300 Loss 1.0259 Accuracy 0.012\n",
            "Epoch 1 Batch 400 Loss 0.8946 Accuracy 0.013\n",
            "Epoch 1 Loss 0.9809\n",
            "Time taken for 1 epoch 352.8883934020996 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.0417 Accuracy 0.013\n",
            "Epoch 2 Batch 100 Loss 0.5331 Accuracy 0.013\n",
            "Epoch 2 Batch 200 Loss 0.6735 Accuracy 0.013\n",
            "Epoch 2 Batch 300 Loss 0.7786 Accuracy 0.014\n",
            "Epoch 2 Batch 400 Loss 0.7218 Accuracy 0.014\n",
            "Epoch 2 Loss 0.6547\n",
            "Time taken for 1 epoch 352.0446343421936 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.8325 Accuracy 0.014\n",
            "Epoch 3 Batch 100 Loss 0.4357 Accuracy 0.014\n",
            "Epoch 3 Batch 200 Loss 0.5647 Accuracy 0.014\n",
            "Epoch 3 Batch 300 Loss 0.6634 Accuracy 0.014\n",
            "Epoch 3 Batch 400 Loss 0.6332 Accuracy 0.014\n",
            "Epoch 3 Loss 0.5475\n",
            "Time taken for 1 epoch 350.7013142108917 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.7129 Accuracy 0.014\n",
            "Epoch 4 Batch 100 Loss 0.3776 Accuracy 0.014\n",
            "Epoch 4 Batch 200 Loss 0.4939 Accuracy 0.014\n",
            "Epoch 4 Batch 300 Loss 0.5765 Accuracy 0.014\n",
            "Epoch 4 Batch 400 Loss 0.5611 Accuracy 0.014\n",
            "Epoch 4 Loss 0.4765\n",
            "Time taken for 1 epoch 350.90468859672546 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.6260 Accuracy 0.014\n",
            "Epoch 5 Batch 100 Loss 0.3273 Accuracy 0.014\n",
            "Epoch 5 Batch 200 Loss 0.4405 Accuracy 0.015\n",
            "Epoch 5 Batch 300 Loss 0.5012 Accuracy 0.015\n",
            "Epoch 5 Batch 400 Loss 0.4875 Accuracy 0.015\n",
            "Epoch 5 Loss 0.4166\n",
            "Time taken for 1 epoch 350.69071221351624 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.5435 Accuracy 0.015\n",
            "Epoch 6 Batch 100 Loss 0.2818 Accuracy 0.015\n",
            "Epoch 6 Batch 200 Loss 0.3741 Accuracy 0.015\n",
            "Epoch 6 Batch 300 Loss 0.4369 Accuracy 0.015\n",
            "Epoch 6 Batch 400 Loss 0.4328 Accuracy 0.015\n",
            "Epoch 6 Loss 0.3634\n",
            "Time taken for 1 epoch 352.3337347507477 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.4741 Accuracy 0.015\n",
            "Epoch 7 Batch 100 Loss 0.2502 Accuracy 0.015\n",
            "Epoch 7 Batch 200 Loss 0.3233 Accuracy 0.015\n",
            "Epoch 7 Batch 300 Loss 0.3738 Accuracy 0.015\n",
            "Epoch 7 Batch 400 Loss 0.3696 Accuracy 0.015\n",
            "Epoch 7 Loss 0.3174\n",
            "Time taken for 1 epoch 351.65111923217773 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.4162 Accuracy 0.015\n",
            "Epoch 8 Batch 100 Loss 0.2156 Accuracy 0.015\n",
            "Epoch 8 Batch 200 Loss 0.2729 Accuracy 0.015\n",
            "Epoch 8 Batch 300 Loss 0.3251 Accuracy 0.015\n",
            "Epoch 8 Batch 400 Loss 0.3261 Accuracy 0.015\n",
            "Epoch 8 Loss 0.2778\n",
            "Time taken for 1 epoch 350.54232907295227 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.3558 Accuracy 0.015\n",
            "Epoch 9 Batch 100 Loss 0.1929 Accuracy 0.015\n",
            "Epoch 9 Batch 200 Loss 0.2369 Accuracy 0.015\n",
            "Epoch 9 Batch 300 Loss 0.2878 Accuracy 0.015\n",
            "Epoch 9 Batch 400 Loss 0.2819 Accuracy 0.015\n",
            "Epoch 9 Loss 0.2433\n",
            "Time taken for 1 epoch 349.48175144195557 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.3044 Accuracy 0.015\n",
            "Epoch 10 Batch 100 Loss 0.1649 Accuracy 0.015\n",
            "Epoch 10 Batch 200 Loss 0.2097 Accuracy 0.015\n",
            "Epoch 10 Batch 300 Loss 0.2502 Accuracy 0.015\n",
            "Epoch 10 Batch 400 Loss 0.2446 Accuracy 0.015\n",
            "Epoch 10 Loss 0.2138\n",
            "Time taken for 1 epoch 349.2764377593994 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.2728 Accuracy 0.015\n",
            "Epoch 11 Batch 100 Loss 0.1442 Accuracy 0.015\n",
            "Epoch 11 Batch 200 Loss 0.1846 Accuracy 0.015\n",
            "Epoch 11 Batch 300 Loss 0.2112 Accuracy 0.015\n",
            "Epoch 11 Batch 400 Loss 0.2088 Accuracy 0.015\n",
            "Epoch 11 Loss 0.1876\n",
            "Time taken for 1 epoch 349.3622303009033 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.2491 Accuracy 0.015\n",
            "Epoch 12 Batch 100 Loss 0.1272 Accuracy 0.015\n",
            "Epoch 12 Batch 200 Loss 0.1590 Accuracy 0.015\n",
            "Epoch 12 Batch 300 Loss 0.1842 Accuracy 0.015\n",
            "Epoch 12 Batch 400 Loss 0.1880 Accuracy 0.015\n",
            "Epoch 12 Loss 0.1648\n",
            "Time taken for 1 epoch 351.50611186027527 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.2184 Accuracy 0.015\n",
            "Epoch 13 Batch 100 Loss 0.1148 Accuracy 0.015\n",
            "Epoch 13 Batch 200 Loss 0.1470 Accuracy 0.015\n",
            "Epoch 13 Batch 300 Loss 0.1641 Accuracy 0.015\n",
            "Epoch 13 Batch 400 Loss 0.1703 Accuracy 0.015\n",
            "Epoch 13 Loss 0.1467\n",
            "Time taken for 1 epoch 351.043256521225 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.1940 Accuracy 0.015\n",
            "Epoch 14 Batch 100 Loss 0.1032 Accuracy 0.015\n",
            "Epoch 14 Batch 200 Loss 0.1337 Accuracy 0.015\n",
            "Epoch 14 Batch 300 Loss 0.1395 Accuracy 0.015\n",
            "Epoch 14 Batch 400 Loss 0.1401 Accuracy 0.015\n",
            "Epoch 14 Loss 0.1310\n",
            "Time taken for 1 epoch 351.008305311203 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.1609 Accuracy 0.015\n",
            "Epoch 15 Batch 100 Loss 0.0974 Accuracy 0.015\n",
            "Epoch 15 Batch 200 Loss 0.1213 Accuracy 0.015\n",
            "Epoch 15 Batch 300 Loss 0.1371 Accuracy 0.015\n",
            "Epoch 15 Batch 400 Loss 0.1362 Accuracy 0.015\n",
            "Epoch 15 Loss 0.1177\n",
            "Time taken for 1 epoch 350.8352861404419 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.1423 Accuracy 0.015\n",
            "Epoch 16 Batch 100 Loss 0.0878 Accuracy 0.015\n",
            "Epoch 16 Batch 200 Loss 0.1150 Accuracy 0.015\n",
            "Epoch 16 Batch 300 Loss 0.1182 Accuracy 0.015\n",
            "Epoch 16 Batch 400 Loss 0.1254 Accuracy 0.015\n",
            "Epoch 16 Loss 0.1064\n",
            "Time taken for 1 epoch 352.1698350906372 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.1476 Accuracy 0.015\n",
            "Epoch 17 Batch 100 Loss 0.0758 Accuracy 0.015\n",
            "Epoch 17 Batch 200 Loss 0.1009 Accuracy 0.015\n",
            "Epoch 17 Batch 300 Loss 0.1138 Accuracy 0.015\n",
            "Epoch 17 Batch 400 Loss 0.1195 Accuracy 0.015\n",
            "Epoch 17 Loss 0.0993\n",
            "Time taken for 1 epoch 351.75908303260803 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.1266 Accuracy 0.015\n",
            "Epoch 18 Batch 100 Loss 0.0687 Accuracy 0.015\n",
            "Epoch 18 Batch 200 Loss 0.0930 Accuracy 0.015\n",
            "Epoch 18 Batch 300 Loss 0.1002 Accuracy 0.015\n",
            "Epoch 18 Batch 400 Loss 0.1084 Accuracy 0.015\n",
            "Epoch 18 Loss 0.0944\n",
            "Time taken for 1 epoch 351.7216022014618 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.1309 Accuracy 0.015\n",
            "Epoch 19 Batch 100 Loss 0.0668 Accuracy 0.015\n",
            "Epoch 19 Batch 200 Loss 0.0827 Accuracy 0.016\n",
            "Epoch 19 Batch 300 Loss 0.0933 Accuracy 0.016\n",
            "Epoch 19 Batch 400 Loss 0.1073 Accuracy 0.016\n",
            "Epoch 19 Loss 0.0894\n",
            "Time taken for 1 epoch 350.3001093864441 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.1259 Accuracy 0.016\n",
            "Epoch 20 Batch 100 Loss 0.0756 Accuracy 0.016\n",
            "Epoch 20 Batch 200 Loss 0.0876 Accuracy 0.016\n",
            "Epoch 20 Batch 300 Loss 0.0944 Accuracy 0.016\n",
            "Epoch 20 Batch 400 Loss 0.1011 Accuracy 0.016\n",
            "Epoch 20 Loss 0.0863\n",
            "Time taken for 1 epoch 350.61907029151917 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IA32Q1SmhiuG"
      },
      "source": [
        "encoder.save_weights('/content/drive/My Drive/encoder_new.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqkt0BRvhxhY"
      },
      "source": [
        "decoder.save_weights('/content/drive/My Drive/decoder_new.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU3Ce8M6I3rz"
      },
      "source": [
        "## Translate\n",
        "\n",
        "* The evaluate function is similar to the training loop, except we don't use *teacher forcing* here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
        "* Stop predicting when the model predicts the *end token*.\n",
        "* And store the *attention weights for every time step*.\n",
        "\n",
        "Note: The encoder output is calculated only once for one input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbQpyYs13jF_"
      },
      "source": [
        "def evaluate(sentence,max_length_targ=100):\n",
        "#   attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "  start_token = [len(tokenizer_en.word_index)+1]\n",
        "  end_token = [len(tokenizer_en.word_index) + 2]\n",
        "  \n",
        "  st=tokenizer_en.texts_to_sequences([sentence.lower()])\n",
        "  flat_list = [item for sublist in st for item in sublist]\n",
        "  inp_sentence = start_token + flat_list + end_token\n",
        "  encoder_input = tf.expand_dims(inp_sentence, 0)\n",
        "  \n",
        "  result = []\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(encoder_input, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([tokenizer_ta.vocab_size+1] , 0)\n",
        "\n",
        "  for t in range(max_length_targ):\n",
        "    predictions, dec_hidden = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    # attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    # attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    \n",
        "    result += [predicted_id]\n",
        "    \n",
        "    if predicted_id == tokenizer_ta.vocab_size+2:\n",
        "      return result, sentence\n",
        "    \n",
        "    \n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3IOI9kxSSaP"
      },
      "source": [
        "def translate(sentence, plot=''):\n",
        "  result,sentence = evaluate(sentence.replace('\\'',' \\''))\n",
        "  \n",
        "  predicted_sentence = tokenizer_ta.decode([i for i in result \n",
        "                                            if i <= tokenizer_ta.vocab_size])  \n",
        "  print(result)\n",
        "  print('Input: {}'.format(sentence))\n",
        "  print('Predicted translation: {}'.format(predicted_sentence))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hj1msVTrVNXC",
        "outputId": "dcdf49b3-3763-47ee-d745-c6e724ebc91b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "translate(\"- NO, HE'S NOT MY DAD.\")\n",
        "print (\"நான் ஞாயிறுகளில் அவளை நாம் மற்ற கூறினார்.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[40, 43, 1, 14, 86, 42, 4, 28, 4, 17, 21, 14, 24, 43, 1, 14, 51, 4411]\n",
            "Input: - NO, HE 'S NOT MY DAD.\n",
            "Predicted translation: - இல்லை, அவர் என் வேலை இல்லை.\n",
            "நான் ஞாயிறுகளில் அவளை நாம் மற்ற கூறினார்.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhM2m819U9_q"
      },
      "source": [
        "encoder.load_weights('/content/drive/My Drive/DEEP LEARNING ASSIGNMENTS/Copy of encoder_new.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfsvqlhiU-RM"
      },
      "source": [
        "decoder.load_weights('/content/drive/My Drive/DEEP LEARNING ASSIGNMENTS/Copy of decoder_new.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTe5P5ioMJwN"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "* [Download a different dataset](http://www.manythings.org/anki/) to experiment with translations, for example, English to German, or English to French.\n",
        "* Experiment with training on a larger dataset, or using more epochs\n"
      ]
    }
  ]
}